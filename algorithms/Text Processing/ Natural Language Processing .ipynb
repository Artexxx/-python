{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of Words или мешок слов —  используеется при обработке текстов, представляет собой неупорядоченный набор слов, входящих в обрабатываемый текст.\n",
    "\n",
    "Часто модель представляют в виде матрицы, в которой строки соответствуют отдельному тексту, а столбцы — входящие в него слова. Ячейки на пересечении являются числом вхождения данного слова в соответствующий документ. Данная модель удобна тем, что переводит человеческий язык слов в понятный для компьтера язык цифр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "raw_documents = ['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two']\n",
    "\n",
    "bag = count.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency TF\n",
    "tf$(t, d)$ частота термина t в документе  d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and        0\n",
       "is         1\n",
       "one        2\n",
       "shining    3\n",
       "sun        4\n",
       "sweet      5\n",
       "the        6\n",
       "two        7\n",
       "weather    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = pd.Series(count.vocabulary_).sort_values()\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>shining</th>\n",
       "      <th>sun</th>\n",
       "      <th>sweet</th>\n",
       "      <th>the</th>\n",
       "      <th>two</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  is  one  shining  sun  sweet  the  two  weather\n",
       "0    0   1    0        1    1      0    1    0        0\n",
       "1    0   1    0        0    0      1    1    0        1\n",
       "2    2   3    2        1    1      1    2    1        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag.toarray(), columns=lexicon.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF   \n",
    "term frequency - inverse document frequency\n",
    "<br>частота терма - обратная частота документа\n",
    ">мера, используемая для снижения веса часто встречающихся слов в векторах\n",
    "\n",
    "**tf-idf** это произведение частоты терма на обратную частоту документа\n",
    "\n",
    "*tf-idf* $(t, d) = \\mathrm{tf}(t, d)*\\mathrm{idf}(t,d)$ \n",
    "<br>${\\displaystyle \\mathrm {idf} (t,d)=\\log {\\frac{N_d}{1+ \\mathrm {df}(d, t)}}}$\n",
    "-  $N_d$ - общее кол-во документов\n",
    "-  $ \\mathrm {df}(d, t)$ -  количество документов содержаших терм "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>shining</th>\n",
       "      <th>sun</th>\n",
       "      <th>sweet</th>\n",
       "      <th>the</th>\n",
       "      <th>two</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and    is  one  shining   sun  sweet   the   two  weather\n",
       "0  0.0  0.43  0.0     0.56  0.56   0.00  0.43  0.00     0.00\n",
       "1  0.0  0.43  0.0     0.00  0.00   0.56  0.43  0.00     0.56\n",
       "2  0.5  0.45  0.5     0.19  0.19   0.19  0.30  0.25     0.19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "pd.DataFrame(tfidf.fit_transform(count.fit_transform(raw_documents)).toarray(),\n",
    "             columns=lexicon.index).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более подробно про нормализацию L2\n",
    "<br> - на примере 3 документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.4, 3. , 3.4, 1.3, 1.3, 1.3, 2. , 1.7, 1.3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(raw_documents)).toarray()[-1]\n",
    "raw_tfidf.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{\\text{norm}} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.4, 0.5, 0.2, 0.2, 0.2, 0.3, 0.3, 0.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "**Задача:**\n",
    "    удалить все знаки препинания, кроме смайлов :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(raw_text):\n",
    "    \"\"\" \n",
    "    Очищает текст.\n",
    "    1. удаление html\n",
    "    2. поиск эмоций\n",
    "    3. удаление посторонних знаков и приведение к нижнему регистру + эмоции\n",
    "    \"\"\"\n",
    "    # cleantext = BeautifulSoup(raw_text, \"lxml\").text\n",
    "    cleantext = re.sub('<[^>]*>', '', raw_text)  # ________________________ парсинг html с re быстро, но плохо\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', cleantext)\n",
    "    text = (re.sub('[\\W]+', ' ', cleantext.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50000 документов ~29.6s, с помощью re ~7s \n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word stemming\n",
    "-сокращает слова до их корневой формы\n",
    "> **Лемматиза́ция** — процесс приведения словоформы к лемме — её нормальной (словарной) форме.\n",
    "> <br>* более затратные алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    \"\"\" Алгоритм стемминга Портера\"\"\"\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/artem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english') # 179 слов\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
    "    if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессионная модель для классификации документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test  = df.loc[25000:, 'review'].values\n",
    "y_test  = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words' : [stop, None],\n",
    "                'vect__tokenizer' : [tokenizer, tokenizer_porter],\n",
    "                   'clf__penalty' : ['l1', 'l2'],\n",
    "                        'clf__C'  : [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words' : [stop, None],\n",
    "                'vect__tokenizer' : [tokenizer, tokenizer_porter],\n",
    "                  'vect__use_idf' : [False],\n",
    "                     'vect__norm' : [None],\n",
    "                   'clf__penalty' : ['l1', 'l2'],\n",
    "                         'clf__C' : [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=1))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно достигнуть точности в 93%, но на это потребуется ~10 минут\n",
    "убить_комп_решётчатым_поиском = False\n",
    "\n",
    "if  убить_комп_решётчатым_поиском:\n",
    "    gs_lr_tfidf.fit(X_train, y_train)\n",
    "    print(f'\\nЛучшие параметры: {gs_lr_tfidf.best_params_}')\n",
    "    print(f'\\nЛучший Результат: {gs_lr_tfidf.best_score_:.2}')\n",
    "    clf = gs_lr_tfidf.best_estimator_\n",
    "    print(f'Тестовый результат:{clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессионная модель для классификации документов\n",
      "Тестовый результат: 89.212%\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf.fit(X_train, y_train)\n",
    "score = lr_tfidf.score(X_test, y_test)\n",
    "print(f'Логистическая регрессионная модель для классификации документов\\nТестовый результат: {score:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внешнее обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    \"\"\" Генератор, читает и возращает по 1 документу из файла\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # пропуск заголовка\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label = next(stream_docs(path = 'movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size= 5000):\n",
    "    \"\"\" Накаливает поток документов до размера size и возращает \"\"\"\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, # n- кол-во признаков\n",
    "                         preprocessor=None, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:10\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45, bar_char= '🙂')\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Работа с большими данными онлайн-алгоритм\n",
      "Тестовый результат: 80.720%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test_raw, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test_raw)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f'Работа с большими данными онлайн-алгоритм\\nТестовый результат: {score:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing estimators\n",
    "Сохранение алгоритма для дальнейшего использования\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf,  open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('movieclassifier')\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказание: positive\n",
      "Вероятность: 79.20%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print(f'Предсказание: {label[clf.predict(X)[0]]}\\nВероятность: {np.max(clf.predict_proba(X)):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематической моделирование\n",
    ">**Задача:**<br>\n",
    "построить модель, которая определяет, к каким темам относится каждый из документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Латентное Размещение Дирихле (LDA)\n",
    "\n",
    "**Идея:**<br>отыскать группы слов, часто появляющиеся вместе в различных документах\n",
    "        <br>- эти слова представляют темы \n",
    "\n",
    "**Алгоритм:** \n",
    "    <br>LDA получает модель суммирования слов и раскладывает её на 2 матрицы\n",
    "    <br>- матрица отображения слов на темы\n",
    "    <br>- матрица отображения документов на темы\n",
    "    \n",
    "  \n",
    "\n",
    "<img src='https://user-images.githubusercontent.com/54672403/82434974-503d6d00-9a9c-11ea-8f42-e41a34325106.png' width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42, learning_method='batch',n_jobs=-1)\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема: 1\n",
      "\ttv school dvd watched women\n",
      "Тема: 2\n",
      "\trole music performance actor play\n",
      "Тема: 3\n",
      "\tbook novel murder version read\n",
      "Тема: 4\n",
      "\thorror sex gore thriller budget\n",
      "Тема: 5\n",
      "\tguy worst stupid game minutes\n",
      "Тема: 6\n",
      "\tfather wife family mother john\n",
      "Тема: 7\n",
      "\tcomedy series kids episode fun\n",
      "Тема: 8\n",
      "\tfeel audience documentary different cinema\n",
      "Тема: 9\n",
      "\twar american men police country\n",
      "Тема: 10\n",
      "\taction effects special budget original\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Тема: {topic_idx + 1}\")\n",
    "    print('\\t'+\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
