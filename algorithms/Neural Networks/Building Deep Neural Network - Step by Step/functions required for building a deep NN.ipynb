{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2(func) import *\n",
    "from smile_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Общий план задания\n",
    "\n",
    "Чтобы построить нейронную сеть, надо реализовывать несколько \"вспомогательных функций\". Эти вспомогательные функции будут использованы в следующем задании для построения двухслойной нейронной сети и L-слойной нейронной сети.\n",
    "\n",
    "- Инициализируйте параметры для двухслойной сети и для $L$-слойной нейронной сети.\n",
    "- Реализовать модуль forward propagation (показан фиолетовым цветом на рисунке ниже).\n",
    "    - Завершите  the part of a LINEAR layer's forward propagation step  (в результате чего $Z^{[l]}$).\n",
    "    - Функцию активации (relu/ sigmoid).\n",
    "    - Объедините предыдущие два шага в новую функцию  [LINEAR->ACTIVATION].\n",
    "    - Сложите  функцию [LINEAR - >RELU] L-1 раз (для слоев с 1 по L-1) и добавьте [LINEAR->SIGMOID] в конце (для конечного слоя $L$). Это дает новую функцию L_model_forward.\n",
    "- Подсчитайте потери.\n",
    "\n",
    "- Реализовать backward propagation module (обозначенный красным цветом на рисунке ниже).\n",
    "    - Complete the LINEAR part of a layer's backward propagation step.\n",
    "    - Мы даем вам градиент функции активации (relu_backward/sigmoid_backward)\n",
    "    - Объедините предыдущие два шага в новую [LINEAR->ACTIVATION] backward функцию.\n",
    "    - Сложите  функцию [LINEAR->RELU]] Л-1 раз и добавить [LINEAR->SIGMOID] backward в новую функцию L_model_backward\n",
    "- Наконец-то обновите параметры.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84135621-e12bb680-aa52-11ea-8991-e16f38d67c0f.png\" style=\"width:800px;height:500px;\">\n",
    "\n",
    "**Обратите внимание**, что для каждой forward функции существует соответствующая backward функция. Именно поэтому на каждом шаге вашего forward модуля вы будете хранить некоторые значения в кэше. Кэшированные значения полезны для вычисления градиентов. Затем в модуле обратного распространения вы будете использовать кэш для вычисления градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Initialization\n",
    "\n",
    "Вы напишете две вспомогательные функции, которые будут инициализировать параметры для вашей модели. Первая функция будет использоваться для инициализации параметров двухслойной модели. Во втором случае этот процесс инициализации будет обобщен на слои $L$.\n",
    "\n",
    "\n",
    "###  2-layer Neural Network\n",
    "\n",
    "Создайте и инициализируйте параметры 2-слойной нейронной сети.\n",
    "\n",
    "**Инструкции**:\n",
    "- Структура модели такова:  *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- Используйте случайную инициализацию для весовых матриц -  `np.random.randn(shape)*0.01`\n",
    "- Используйте нулевую инициализацию для смещений - `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    returns: parameters (dict) содержащий параметры:\n",
    "                    W1 -- weight (matrix) shape (n_h, n_x)\n",
    "                    b1 -- bias (vector)   shape (n_h, 1)\n",
    "                    W2 -- weight (matrix) shape (n_y, n_h)\n",
    "                    b2 -- bias (vector)   shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1,\n",
    "                  \"W2\": W2, \"b2\": b2}\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W1 = \n",
      " [[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]]\n",
      "\n",
      "b1 = [0.] [0.]\n",
      "W2 =  [[ 0.00865408 -0.02301539]]\n",
      "b2 =  [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(2,2,1)\n",
    "print (\"\\nW1 = \\n\", parameters[\"W1\"])\n",
    "print (\"\\nb1 =\", *parameters[\"b1\"])\n",
    "print (\"W2 = \", parameters[\"W2\"])\n",
    "print (\"b2 = \", parameters[\"b2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  L-layer Neural Network\n",
    "\n",
    "Инициализация для более глубокой L-слойной нейронной сети является более сложной, поскольку существует гораздо больше весовых матриц и векторов смещения. При заполнении `initialize_parameters_deep` вы должны убедиться, что ваши размеры совпадают между каждым слоем. Напомним, что $n^{[l]}$ - это количество единиц измерения в слое $l$. Так, например, если размер наших входных данных $X$ равен $(12288, 209)$ (с примерами $m=209$), то:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Помните, что когда мы вычисляем $W X + b$ в python, он выполняет carries out broadcasting. Например, если:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Тогда $WX + b$ будет:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "реализация инициализации для нейронной сети L-уровня.\n",
    "\n",
    "**Инструкции**:\n",
    "- Структура модели *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. То есть она имеет слои $L-1$, использующие функцию активации ReLU, за которой следует выходной слой с сигмовидной функцией активации.\n",
    "- Используйте случайную инициализацию для весовых матриц  `np.random.rand(shape) * 0.01`.\n",
    "- Используйте инициализацию нулей для смещений  `np.zeros(shape)`.\n",
    "- Мы будем хранить $n^{[l]}$, количество единиц измерения в разных слоях, в переменной `layer_dims`. Например, `layer_dims` для \"Planar Data classification model\" был бы [2,4,1]: там было два входа, один скрытый слой с 4 скрытыми единицами и выходной слой с 1 выходной единицей. Таким образом, значит и `W1` форма была (4,2), `В1` был (4,1), `В2` был (1,4) и `В2` был (1,1). Теперь вы обобщите это до$ L $ слоев!\n",
    "- Вот реализация для $L=1$ (однослойная нейронная сеть). Это должно вдохновить вас на реализацию общего случая (L-слойная нейронная сеть).\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims (list) cодержит размеры каждого слоя в нашей сети\n",
    "    \n",
    "    returns: parameters (dict) содержащий параметры \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight (matrix) shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias (vector)   shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # количество слоев в сети\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W1 = \n",
      " [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "\n",
      "b1 = [0.] [0.] [0.] [0.]\n",
      "W2 = \n",
      " [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 =  [0.] [0.] [0.]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print (\"\\nW1 = \\n\", parameters[\"W1\"])\n",
    "print (\"\\nb1 =\", *parameters[\"b1\"])\n",
    "print (\"W2 = \\n\", parameters[\"W2\"])\n",
    "print (\"b2 = \",*parameters[\"b2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Forward propagation module\n",
    "\n",
    "### Linear Forward \n",
    "Теперь, когда вы инициализировали свои параметры, вы сделаете forward propagation module. Вы начнете с реализации некоторых основных функций, которые вы будете использовать позже при реализации модели. Вы будете выполнять три функции в таком порядке:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
    "\n",
    "Linear forward module векторизованный по всем примерам) вычисляет следующие уравнения:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "где $A^{[0]} = X$. \n",
    "\n",
    "\n",
    "постройте линейную часть прямого распространения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    линейная часть forward propagation.\n",
    "\n",
    "    A -- активации с предыдущего слоя (или input data): (size of previous layer, number of examples)\n",
    "    W -- weights (matrix): numpy array shape (size of current layer, size of previous layer)\n",
    "    b -- bias (vector), numpy array    shape (size of the current layer, 1)\n",
    "\n",
    "    returns: \n",
    "        Z -- входной сигнал функции активации, также называемый параметром предварительной активации\n",
    "        cache (dict) содержащий \"A\", \" W \" и \"b\"; хранится для эффективного вычисления backward\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-Activation Forward\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Мы снабдили вас функцией `sigmoid`. Эта функция возвращает два элемента: значение активации `A` и `кэш`, содержащий `Z` (это то, что мы будем подавать в соответствующую backward функцию).\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: математическая формула для ReLu $A = RELU(Z) = max(0, Z)$. Мы предоставили вам функцию `relu`. Эта функция возвращает два элемента: значение активации `A` и `кэш`, содержащий `Z` (это то, что мы будем подавать в соответствующую backward функцию). \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большего удобства вы сгруппируете две функции (Линейная и активационная) в одну функцию (LINEAR->ACTIVATION). Таким образом, вы реализуете функцию, которая выполняет LINEAR forward шаг, за которым следует шаг ACTIVATION forward\n",
    "\n",
    "**Упражнение**: реализуйте прямое распространение слоя *LINEAR->ACTIVATION*. Математическое соотношение таково: $A^{[l]} = g (Z^{[l]}) = g (W^{[l]}A^{[l-1]} +b^{[l]})$, где активация \"g\" может быть sigmoid() или relu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    forward propagation для LINEAR->ACTIVATION layer\n",
    "\n",
    "    A_prev -- активации с предыдущего слоя (или input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array  shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array     shape (size of the current layer, 1)\n",
    "    activation -- {\"sigmoid\"|\"relu\"} активация, которая будет использоваться в этом слое \n",
    "\n",
    "    returns:\n",
    "        A  -- вывод функции активации, также называемой значением после активации\n",
    "        cache -- (dict) содержащий \"linear_cache\" и \" activation_cache\";\n",
    "            сохраненный для эффективного вычисления backward \n",
    "    \"\"\"\n",
    "    \n",
    "    # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "     \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A =  [0.96890023 0.11013289]\n",
      "With ReLU: A =  [3.43896131 0.        ]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" , *A)\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" , *A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**: в глубоком обучении вычисление \"[линейная->активация] \" считается одним слоем в нейронной сети, а не двумя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Layer Model \n",
    "\n",
    "Для еще большего удобства при реализации нейронной сети уровня $L$вам понадобится функция, которая повторяет предыдущую (`linear_activation_forward` с RELU) $L-1$ раз, а затем следует за ней с одним `linear_activation_forward` с СИГМОИДОМ.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84140241-f8ba6d80-aa59-11ea-813e-fb94dc459a06.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center>  *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "**Упражнение**: реализуйте прямое распространение приведенной выше модели.\n",
    "\n",
    "**Инструкция**: В приведенном ниже коде переменная `AL` будет обозначать $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma (W^{[L]} A^{[L-1]} + b^{[L]})$. (Это иногда также называется `Yhat`, т.е. это $\\hat{Y}$.)\n",
    "\n",
    "\n",
    "- Используйте функции, которые вы ранее написали\n",
    "- Используйте цикл for для [LINEAR->RELU] (L-1) раз\n",
    "- Не забывайте следить за cach в списке \"caches\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Прямое распространение для [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID \n",
    "    \n",
    "    X -- (matrix) shape (input size, number of examples)\n",
    "    parameters -- вывод initialize_parameters_deep()\n",
    "    \n",
    "    returns:\n",
    "        AL -- последнее значение post-activation\n",
    "        caches (list) --\n",
    "            каждый cache из linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                   cache из linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # количество слоев в нейронной сети\n",
    "    \n",
    "    # [LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev,parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # LINEAR -> SIGMOID\n",
    "    AL, cache = linear_activation_forward(A,parameters['W' + str(L)],parameters['b' + str(L)],activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL =  [0.17007265 0.2524272 ]\n",
      "Length of caches list =  2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \", *AL)\n",
    "print(\"Length of caches list = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь у вас есть полное forward propagation, которое принимает вход X и выводит вектор строки $A^{[L]}$, содержащий ваши предсказания. Он также записывает все промежуточные значения в \"кэши\". Используя $A^{[L]}$, вы можете вычислить стоимость ваших прогнозов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Теперь вы будете осуществлять прямое и обратное распространение. Вам нужно вычислить стоимость, потому что вы хотите проверить, действительно ли ваша модель учится.\n",
    "\n",
    "**Упражнение**: вычислите стоимость кросс-энтропии $J$, используя следующую формулу:$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- (vector) {0:non-cat, 1: cat}, shape (1, number of examples)\n",
    "\n",
    "    returns: cost - cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # потери от aL и Y.\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL),axis=1,keepdims=True)\n",
    "    \n",
    "    cost = np.squeeze(cost) \n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation module\n",
    "\n",
    "Так же, как и при прямом распространении, вы будете реализовывать вспомогательные функции для обратного распространения. Помните, что обратное распространение используется для вычисления градиента функции потерь по отношению к параметрам.\n",
    "\n",
    "**Reminder**: \n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84146163-e8a78b80-aa63-11ea-9c1a-140641ba94fa.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> Forward and Backward propagation для *LINEAR->RELU->LINEAR->SIGMOID* <br> *Фиолетовые блоки представляют собой прямое распространение, а красные блоки-обратное распространение.*  </center></caption>\n",
    "\n",
    "\n",
    ">цепное правило исчисления может быть использовано для получения производной от потери $\\mathcal{L}$ по отношению к $z^{[1]}$ в 2-слойной сети следующим образом:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "Чтобы вычислить градиент $dW^{[1]} = \\frac {\\partial L}{\\partial W^{[1]}}$, вы используете предыдущее цепное правило и делаете $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]}} {\\partial W^{[1]}}$. Во время обратного распространения, на каждом шаге вы умножаете свой текущий градиент на градиент, соответствующий определенному слою, чтобы получить нужный вам градиент.\n",
    "\n",
    "Эквивалентно, чтобы вычислить градиент $db^{[1]} = \\frac {\\partial L}{\\partial b^{[1]}}$, вы используете предыдущее правило цепочки и делаете $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]}} {\\partial b^{[1]}}$.\n",
    "\n",
    "Вот почему мы говорим о **обратном распространении**.\n",
    "\n",
    "\n",
    "Теперь, подобно прямому распространению, вы собираетесь построить обратное распространение в три этапа:\n",
    "- Линейная обратная связь (LINEAR backward)\n",
    "- LINEAR -> ACTIVATION backward, где активация вычисляет производную либо от ReLU, либо от сигмовидной активации\n",
    "-  [LINEAR -> RELU]  $\\times$ (L-1) -> LINEAR -> SIGMOID backward(вся модель)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear backward\n",
    "\n",
    "Для слоя $l$ линейная часть равна: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (после чего следует активация).\n",
    "\n",
    "Предположим, что вы уже вычислили производную $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$.Вы хотите получить $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84146815-f1e52800-aa64-11ea-9aff-b80e587be556.png\" style=\"width:250px;height:300px;\">\n",
    "\n",
    "Tри выхода $(dW^{[l]}, db^{[l]}, dA^{[l]})$ вычисляются с использованием входных данных $dZ^{[l]}$.Вот необходимые вам формулы:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Линейная часть обратного распространения для одного слоя (слой l)\n",
    "\n",
    "    dZ -- градиент стоимости по отношению к линейному выходу (текущего слоя l)\n",
    "    cache -- кортеж значений (A_prev, W, b), поступающих из прямого распространения в текущем слое\n",
    "\n",
    "    returns:\n",
    "        dA_prev - градиент стоимости по отношению к активации (предыдущего слоя l-1), такой же формы, как и A_prev\n",
    "        db - градиент стоимости по отношению к b (текущий слой l), такой же формы, как и b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1 / m * np.dot(dZ ,A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ,axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ) \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = \n",
      " [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "\n",
      "dW =  [-0.10076895  1.40685096  1.64992505]\n",
      "db =  [0.50629448]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \\n\", dA_prev)\n",
    "print (\"\\ndW = \", *dW)\n",
    "print (\"db = \", *db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear-Activation backward\n",
    "\n",
    "Затем вы создадите функцию, которая объединяет две вспомогательные функции: **`linear_backward`** и обратный шаг для активации **`linear_activation_backward'**.\n",
    "\n",
    "Чтобы помочь вам реализовать ' linear_activation_backward`, мы предоставили две обратные функции:\n",
    "- **`sigmoid_backward`**: реализует обратное распространение для сигмовидной единицы.\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Реализует обратное распространение для блока RELU.\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "Если $g(.)$ - это функция активации,\n",
    "`sigmoid_backward` и `relu_backward` вычислят $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$.  \n",
    "\n",
    "Реализовать метод обратного распространения ошибки для слоя *LINEAR->ACTIVATION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    метод обратного распространения ошибки для слоя *LINEAR->ACTIVATION*\n",
    "    \n",
    "    dA -- post-activation градиент для текущего слоя l\n",
    "    cache -- кортеж значений (linear_cache, activation_cache) мы храним для эффективного вычисления обратного распространения\n",
    "    activation -- {\"sigmoid\", \"relu\"} активация, которая будет использоваться в этом слое\n",
    "    \n",
    "    returns:\n",
    "        dA_prev -- градиент стоимости по отношению к активации (предыдущего слоя l-1), такой же формы, как и A_prev\n",
    "        dW -- градиент стоимости по отношению к активации (предыдущего слоя l-1), такой же формы, как и A_prev\n",
    "        db -- градиент стоимости по отношению к b (текущий слой l), такой же формы, как и b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = \n",
      " [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "\n",
      "dW =  [ 0.10266786  0.09778551 -0.01968084]\n",
      "db =  [-0.05729622]\n",
      "\n",
      "relu:\n",
      "dA_prev = \n",
      " [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "\n",
      "dW =  [ 0.44513824  0.37371418 -0.10478989]\n",
      "db =  [-0.20837892]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \\n\", dA_prev)\n",
    "print (\"\\ndW = \", *dW)\n",
    "print (\"db = \", *db)\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"\\nrelu:\")\n",
    "print (\"dA_prev = \\n\", dA_prev)\n",
    "print (\"\\ndW = \", *dW)\n",
    "print (\"db = \", *db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Model Backward \n",
    "\n",
    "Теперь вы будете реализовывать обратную функцию для всей сети. Напомним, что при реализации функции `L_model_forward` на каждой итерации вы сохраняли кэш,содержащий (X,W, b и z). В модуле обратного распространения вы будете использовать эти переменные для вычисления градиентов. Поэтому в функции `L_model_backward` вы будете перебирать все скрытые слои в обратном порядке, начиная со слоя $L$. На каждом шаге вы будете использовать кэшированные значения для слоя $l$ для обратного распространения через слой $l$. На рисунке ниже показан обратный проход.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84168073-a68c4300-aa7f-11ea-9adf-7a86ff50eed1.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> Backward pass  </center></caption>\n",
    "\n",
    "** Инициализация обратного распространения**:\n",
    "Для обратного распространения через эту сеть мы знаем, что выход является,\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Таким образом, ваш код должен вычислить `dAL ' $ = \\frac {\\partial \\mathcal{L}} {\\partial A^{[L]}}$.\n",
    "Для этого используйте эту формулу (полученную с помощью исчисления, в котором вам не нужны глубокие знания):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # производная от стоимости по отношению к AL\n",
    "```\n",
    "Затем вы можете использовать этот градиент после активации `dAL`, чтобы продолжать движение назад. Как видно на Рисунке, Теперь вы можете ввести `dAL` в реализованную вами функцию LINEAR->SIGMOID backward (которая будет использовать кэшированные значения, сохраненные функцией L_model_forward). После этого вам придется использовать цикл `for` для итерации по всем другим слоям с помощью функции LINEAR->RELU backward. Вы должны хранить каждый dA, dW и db в словаре grads. Для этого используйте следующую формулу :\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]} $$\n",
    "Например, для $l=3$ будет сохранено $dW^{[l]}$ в  `grads[\"dW3\"]`.\n",
    "\n",
    "**реализация обратного распространения для *[LINEAR->RELU] $\\times$(L-1) -> LINEAR -> SIGMOID* model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    backward propagation для [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    \n",
    "    AL - вероятностный вектор, выходной сигнал прямого распространения (L_model_forward())\n",
    "    Y  - vector целевых переменных {0 - non-cat, 1-cat}\n",
    "    caches -- список кэшов, содержащих:\n",
    "        каждый кэш linear_activation_forward() с \"relu\" (это кэш[l], for l in range(L-1) т.e l = 0...L-2)\n",
    "        кэш linear_activation_forward () с \"sigmoid\" (это кэш[L-1])\n",
    "    \n",
    "    returns: grads (dict) с градиентами\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # количество слоев\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # после этой линии Y имеет ту же форму, что и AL\n",
    "    \n",
    "    # Initializing backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    ## Lth layer (SIGMOID -> LINEAR) gradients. \n",
    "    # Inputs: \"AL, Y, caches\". \n",
    "    # Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    (grads[\"dA\" + str(L)],\n",
    "     grads[\"dW\" + str(L)],\n",
    "     grads[\"db\" + str(L)]) = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        ## lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\".\n",
    "        # Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        (grads[\"dA\" + str(l + 1)],\n",
    "         grads[\"dW\" + str(l + 1)],\n",
    "         grads[\"db\" + str(l + 1)]) = \\\n",
    "        linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, activation = \"relu\")\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = \n",
      " [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "\n",
      " db1 =\n",
      " [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "\n",
      " db1 = \n",
      " [[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "\n",
    "print (\"dW1 = \\n\", grads[\"dW1\"])\n",
    "print (\"\\n db1 =\\n\",grads[\"db1\"])\n",
    "print (\"\\n db1 = \\n\", grads[\"dA1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "\n",
    "В этом разделе вы будете обновлять параметры модели, используя градиентный спуск:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
    "\n",
    "где $\\alpha$ - это скорость обучения. После вычисления обновленных параметров сохраните их в словаре параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Инструкции**:\n",
    "Обновите параметры с помощью градиентного спуска на каждом $W^{[l]}$ и $b^{[l]}$ для $l = 1, 2, ..., L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Обновление параметров с помощью градиентного спуска\n",
    "\n",
    "    parameters - (dict), содержащий параметры\n",
    "    grads - (dict), содержащий градиенты, (вывод L_model_backward)\n",
    "    \n",
    "    returns: parameters - (dict), содержащий обновленные параметры\n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # количество слоев в нейронной сети\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] =  parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = \n",
      " [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "\n",
      "b1 = \n",
      " [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "\n",
      "W2 =  [-0.55569196  0.0354055   1.32964895]\n",
      "\n",
      "b2 =  [-0.84610769]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \\n\", parameters[\"W1\"])\n",
    "print (\"\\nb1 = \\n\", parameters[\"b1\"])\n",
    "print (\"\\nW2 = \", *parameters[\"W2\"])\n",
    "print (\"\\nb2 = \", *parameters[\"b2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions required for building a deep neural net-\n",
    "work"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
