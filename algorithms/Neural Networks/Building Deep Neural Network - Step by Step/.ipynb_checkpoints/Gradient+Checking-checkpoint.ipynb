{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "\n",
    "Вы являетесь частью команды, работающей над тем, чтобы сделать мобильные платежи доступными по всему миру, и вас просят построить модель глубокого обучения для обнаружения мошенничества-всякий раз, когда кто-то делает платеж, вы хотите увидеть, может ли платеж быть мошенническим, например, если учетная запись пользователя была захвачена хакером.\n",
    "\n",
    "Но обратное распространение довольно сложно реализовать, и иногда у него есть ошибки. Поскольку это критически важное приложение, генеральный директор вашей компании хочет быть действительно уверен, что ваша реализация обратного распространения является правильной. Ваш генеральный директор говорит: \"Дайте мне доказательство того, что ваше обратное распространение действительно работает!\"Чтобы дать это заверение, вы собираетесь использовать \"проверку градиента\".\n",
    "\n",
    "Давайте сделаем это!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-33b9c1e71fe1>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-33b9c1e71fe1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from testCases_v3(reg_check) import *\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from testCases_v3(reg_check) import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does gradient checking work?\n",
    "\n",
    "Обратное распространение вычисляет градиенты $\\frac {\\partial J}{\\partial \\theta}$, где $\\theta$ обозначает параметры модели. $J$ вычисляется с использованием прямого распространения и вашей функции потерь.\n",
    "\n",
    "Поскольку прямое распространение относительно легко реализовать, вы уверены, что сделали это правильно, и поэтому вы почти на 100% уверены, что правильно вычисляете стоимость $J$. Таким образом, вы можете использовать ваш код для вычисления $J$ для проверки кода для вычисления $\\frac{\\partial J}{\\partial \\theta}$. \n",
    "\n",
    "Давайте вернемся к определению производной (или градиента):\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Если вы не знакомы с этим \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$ \"нотация, это просто способ сказать: \"когда $\\varepsilon$ действительно очень мал.\"\n",
    "\n",
    "Мы знаем следующее:\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ это то, что вы хотите, чтобы убедиться, что вы вычисляете правильно.\n",
    "- Вы можете вычислить $J(\\theta + \\varepsilon)$ и $J(\\theta - \\varepsilon)$ (в случае, если $\\theta$ это реальная цифра), поскольку вы уверены, что ваши реализации за $J$ является правильным.\n",
    "\n",
    "Давайте используем уравнение (1) и небольшое значение для $\\varepsilon$, чтобы убедить вашего генерального директора в том, что ваш код для вычислений $\\frac{\\partial J}{\\partial \\theta}$ правелен!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1-dimensional gradient checking\n",
    "\n",
    "Рассмотрим 1D линейную функцию $J (\\theta) = \\theta x$. Модель содержит только один вещественный параметр $\\theta$ и принимает $x$ в качестве входных данных.\n",
    "\n",
    "Вы реализуете код для вычисления $J(.)$ и ее производной$\\frac{\\partial J}{\\partial \\theta}$ Затем вы будете использовать проверку градиента, чтобы убедиться, что ваше производное вычисление для $J$ правильно.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84299844-1242de80-ab5a-11ea-815f-b075cc8d4742.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> **1D linear model**<br> </center></caption>\n",
    "\n",
    "На приведенной выше диаграмме показаны основные шаги вычисления: сначала начните с $x$, а затем оцените функцию $J(x)$ (\"прямое распространение\"). Затем вычислите производную $\\frac {\\partial J}{\\partial \\theta}$ (\"обратное распространение\").\n",
    "\n",
    "**Упражнение**: реализуйте \"прямое распространение\" и \"обратное распространение\" для этой простой функции. То есть, вычислите оба $J(.)$ (\"прямое распространение\") и его производную по отношению к $\\theta$ (\"обратное распространение\"), в двух отдельных функциях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Реализуйте линейное прямое распространение (compute J) представлен на Figure 1 (J(theta) = theta * x)\n",
    "    \n",
    "    x - реальный входной сигнал\n",
    "    theta - наш параметр, а также вещественное число\n",
    "    \n",
    "    returns: J - значение функции J, вычисленное по формуле J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    J = theta * x\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J =  8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: теперь выполните шаг обратного распространения (производное вычисление) рисунка 1. То есть вычислите производную от $J (\\theta) = \\theta x$ по отношению к $\\theta$. Чтобы избавить вас от выполнения вычислений, вы должны получить $dtheta = \\frac {\\partial J }{ \\partial \\theta} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Вычисляет производную от J относительно theta (Figure 1).\n",
    "    \n",
    "    x - реальный входной сигнал\n",
    "    theta - наш параметр, а также вещественное число\n",
    "    \n",
    "    returns: dtheta - градиент стоимости по отношению к theta\n",
    "    \"\"\"\n",
    "    dtheta = x\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: чтобы показать, что функция `backward_propagation()` правильно вычисляет градиент $\\frac{\\partial J}{\\partial \\theta}$, давайте реализуем проверку градиента.\n",
    "\n",
    "**Инструкции**:\n",
    "- Сначала вычислите \"gradapprox\", используя формулу выше (1) и небольшое значение $\\varepsilon$. Вот шаги, которые необходимо выполнить:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- Затем вычислите градиент, используя обратное распространение, и сохраните результат в переменной \"grad\"\n",
    "- Наконец, вычислите относительную разницу между \"gradapprox\" и \"grad\", используя следующую формулу:\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "Вам понадобится 3 шага, чтобы вычислить эту формулу:\n",
    "    - 1'. вычислите числитель, используяnp.linalg.norm(...)\n",
    "    - 2'. вычислите знаменатель. Вам нужно будет вызвать np.linalg.norm(...) дважды.\n",
    "    - 3'. разделите их.\n",
    "- Если эта разница невелика (скажем, меньше $10^{-7}$), вы можете быть вполне уверены, что правильно рассчитали свой градиент. В противном случае, возможно, будет допущена ошибка в вычислении градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Обратное распространение, представленное на рисунке 1.\n",
    "\n",
    "    x - реальный входной сигнал\n",
    "    theta - наш параметр, а также вещественное число\n",
    "    epsilon - крошечный сдвиг на вход для вычисления аппроксимированного градиента формулой(1)\n",
    "    \n",
    "    returns: difference -разница (2) между аппроксимированным градиентом и обратным градиентом распространения\n",
    "    \"\"\"\n",
    "    \n",
    "    # Вычислите gradapprox, используя левую часть Формулы (1).\n",
    "    ## Эпсилон достаточно мал, вам не нужно беспокоиться о пределе.\n",
    "    thetaplus = theta + epsilon                               # Step 1\n",
    "    thetaminus = theta - epsilon                              # Step 2\n",
    "    J_plus = forward_propagation(x, thetaplus)                # Step 3\n",
    "    J_minus = forward_propagation(x, thetaminus)              # Step 4\n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)           # Step 5\n",
    "    \n",
    "    # Проверьте, достаточно ли близко gradapprox находится к выходу backward_propagation()\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "    ## approx\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                  # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)# Step 2'\n",
    "    difference = numerator / denominator                           # Step 3'\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print (\"The gradient is correct!\")\n",
    "    else:\n",
    "        print (\"The gradient is wrong!\")\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference =  2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \", difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поздравляю, разница меньше, чем порог $10^{-7}$. Таким образом, у вас может быть высокая уверенность в том, что вы правильно вычислили градиент в `backward_propagation()`.\n",
    "\n",
    "Теперь, в более общем случае, ваша функция затрат $J$ имеет более одного входа 1D. Когда вы тренируете нейронную сеть, $\\theta$ на самом деле состоит из нескольких матриц $W^{[l]}$ и смещений $b^{[l]}$! Важно знать, как выполнить проверку градиента с помощью более объемных входных данных. Давайте сделаем это!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  N-dimensional gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "На следующем рисунке описывается прямое и обратное распространение вашей модели обнаружения мошенничества.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84302593-3a344100-ab5e-11ea-9737-d218370297d0.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **deep neural network**<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*</center></caption>\n",
    "\n",
    "Давайте рассмотрим ваши реализации для прямого распространения и обратного распространения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Реализует прямое распространение (и вычисляет стоимость), представленное на рисунке 2.\n",
    "     LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "   \n",
    "    X -- обучающий набор для m примеров\n",
    "    Y -- метки классов для примеров м\n",
    "    parameters -- (dict) содержащий параметры \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix shape (5, 4)\n",
    "                    b1 -- bias vector   shape (5, 1)\n",
    "                    W2 -- weight matrix shape (3, 5)\n",
    "                    b2 -- bias vector   shape (3, 1)\n",
    "                    W3 -- weight matrix shape (1, 3)\n",
    "                    b3 -- bias vector   shape (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for one example)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    # Cost\n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обратное распространение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Реализуйте обратное распространение, представленное на рисунке 2.\n",
    "\n",
    "    X -- входные данные, shape (input size, 1)\n",
    "    Y -- метки классов\n",
    "    cache -- вывод кэша из forward_propagation_n()\n",
    "    \n",
    "    returns: gradients (dict)  градиенты стоимости по каждому параметру, активационным и предактивационным переменным.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)*2\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Осторожно, решение!!!__\n",
    "<font color='E3E3E3'>\n",
    "dW2 = 1. / m * np.dot(dZ2, A1.T) * 2  # Should not multiply by 2<br>\n",
    "db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True) # Should not multiply by 4\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Вы получили некоторые результаты на тестовом наборе обнаружения мошенничества, но вы не уверены на 100% в своей модели. Никто не совершенен! Давайте внедрим проверку градиентов, чтобы проверить, правильны ли ваши градиенты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does gradient checking work?**.\n",
    "\n",
    "Как и в 1) и 2), вы хотите сравнить \"gradapprox\" с градиентом, вычисленным обратным распространением. Формула остается прежней:\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Однако $\\theta$ больше не является скаляром. Это словарь под названием  \"parameters\". Мы реализовали для вас функцию \"`dictionary_to_vector ()`\". Он преобразует словарь  \"parameters\" в Вектор под названием \"values\", полученный путем преобразования всех параметров (W1, b1, W2, b2, W3, b3) в векторы и их объединения.\n",
    "\n",
    "Обратная функция - это \"vector_to_dictionary\", который выводит обратно данные в словарь \"parameters\"\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/54672403/84303652-035f2a80-ab60-11ea-96e6-2e3e1695c501.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **dictionary_to_vector() и vector_to_dictionary()**<br>Вам понадобятся эти функции в gradient_check_n()</center></caption>\n",
    "\n",
    "Мы также преобразовали словарь \"gradients \"в вектор \"grad\" с помощью функции gradients_to_vector(). Вам не нужно беспокоиться об этом.\n",
    "\n",
    "**Упражнение**: реализация функции gradient_check_n ().\n",
    "\n",
    "**Инструкции**: вот псевдокод, который поможет вам реализовать проверку градиента.\n",
    "\n",
    "For each i in num_parameters:\n",
    "- To compute `J_plus[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- To compute `J_minus[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Таким образом, вы получаете вектор gradapprox, где gradapprox[i] - это аппроксимация градиента относительно `parameter_values[i]`. Теперь вы можете сравнить этот вектор gradapprox с вектором градиентов из обратного распространения. Так же, как и для случая 1D (шаги 1', 2', 3'), вычислить:\n",
    "\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Проверяет, правильно ли backward_propagation_n вычисляет градиент выходных данных затрат по forward_propagation_n\n",
    "    parameters -- (dict) содержащий параметры \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- вывод backward_propagation_n, содержит градиенты стоимости по отношению к параметрам.\n",
    "    X -- входные данные, shape (input size, 1)\n",
    "    Y -- метки классов\n",
    "    epsilon -- крошечный сдвиг на вход для вычисления аппроксимированного градиента с помощью formula(1)\n",
    "    \n",
    "    returns: difference -разница (2) между аппроксимированным градиентом и обратным градиентом распространения\n",
    "    \"\"\"\n",
    "    # Настройка переменных\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    #print(\"parameters_values:\", parameters_values)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    #print(\"grad:\", grad)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    ## gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        '''\n",
    "        J_plus[i]\n",
    "         Inputs: \"parameters_values, epsilon\".\n",
    "         Output = \"J_plus[i]\".\n",
    "        '''\n",
    "        thetaplus = np.copy(parameters_values)                              # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                         # Step 2\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))# Step 3\n",
    "        '''\n",
    "        J_minus[i]\n",
    "          Inputs: \"parameters_values, epsilon\".\n",
    "          Output = \"J_minus[i]\".\n",
    "        '''\n",
    "        thetaminus = np.copy(parameters_values)                                     # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                               # Step 2        \n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))# Step 3\n",
    "\n",
    "        ## gradapprox[i]\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "    \n",
    "    # Сравните gradapprox с обратными градиентами распространения, вычисляя разницу\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                       # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)     # Step 2'\n",
    "    difference = numerator / denominator                                # Step 3'\n",
    "\n",
    "    if difference > 1.2e-7:\n",
    "        print (\"\\033[93m\" + \"Есть ошибка в обратном распространении! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Ваше обратное распространение работает совершенно нормально! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mВаше обратное распространение работает совершенно нормально! difference = 1.1890913023330276e-07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, что в коде backward_propagation_n, который мы вам дали, были ошибки! Хорошо, что вы реализовали проверку градиента. Вернитесь к `backward_propagation` и попробуйте найти / исправить ошибки *(подсказка: проверьте dW2 и db1)*. Повторите проверку градиента, когда вы думаете, что исправили его.\n",
    "\n",
    "Можете ли вы получить проверку градиента, чтобы объявить ваше производное вычисление правильным? \n",
    "\n",
    "\n",
    "**Записка**\n",
    "- Проверка градиента идет медленно! Аппроксимации градиента с  $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ является вычислительно затратным. По этой причине мы не выполняем проверку градиента на каждой итерации во время обучения.Нужно время, чтобы проверить, правилен ли градиент.\n",
    "- Проверка градиента, по крайней мере, как мы ее представили, не работает с dropout. Обычно вы запускаете алгоритм проверки градиента без отсева, чтобы убедиться, что ваш backprop верен, а затем добавляете отсев.\n",
    "\n",
    "Поздравляю, вы можете быть уверены, что ваша модель глубокого обучения для обнаружения мошенничества работает правильно! Вы даже можете использовать это, чтобы убедить своего генерального директора. :)\n",
    "\n",
    "<font color='blue'>\n",
    "**Что вы должны помнить из этой тетради**:\n",
    "- Проверка градиента проверяет близость между градиентами от обратного распространения и численной аппроксимацией градиента (вычисленной с использованием прямого распространения).\n",
    "- Проверка градиента выполняется медленно, поэтому мы не запускаем его в каждой итерации обучения. Обычно вы запускаете его только для того, чтобы убедиться, что ваш код верен, а затем выключаете его и используете backprop для реального процесса обучения."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
